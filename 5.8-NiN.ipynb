{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义NiN块，一个自定义的卷积层 + 两个1x1卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, 1), \n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, 1), \n",
    "                        nn.ReLU())\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NiN模型\n",
    "\n",
    "使用全局平均池化代替全连接层（池化层核等于输入的高和宽，通道数为10）\n",
    "\n",
    "作用：减少参数尺寸，缓解过拟合（有时会造成获得有效模型的训练时间的增加）\n",
    "\n",
    "![NiN](./images/NiN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义全局平均池化\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self, img):\n",
    "        return F.avg_pool2d(img, kernel_size=img.size()[2:])  # 除开前面的batch和channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NiN, self).__init__()\n",
    "        self.conv_fc = nn.Sequential(\n",
    "                        nin_block(1, 96, kernel_size=11, stride=4, padding=0),  # 96x54x54 （省略第一维的batch_size）\n",
    "                        nn.MaxPool2d(kernel_size=3, stride=2),  # 96x26x26\n",
    "                        nin_block(96, 256, kernel_size=5, stride=1, padding=2),  # 256x26x26 \n",
    "                        nn.MaxPool2d(kernel_size=3, stride=2),  # 256x12x12\n",
    "                        nin_block(256, 384, kernel_size=3, stride=1, padding=1),  # 384x12x12\n",
    "                        nn.MaxPool2d(kernel_size=3, stride=2),  # 384x5x5\n",
    "                        nn.Dropout(0.5), \n",
    "        \n",
    "                        nin_block(384, 10, kernel_size=3, stride=1, padding=1),  # 10x5x5\n",
    "                        GlobalAvgPool2d(),  # 经过全局平均池化后是 batch_size x 10(C) x 1(H) x 1(W)\n",
    "                        d2l.FlattenLayer())  # 转换为 batch_size x 10\n",
    "        \n",
    "    def forward(self, img):\n",
    "        return self.conv_fc(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NiN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个数据样本来查看每一层的输出形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_fc output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 1, 224, 224)\n",
    "for name, blk in net.named_children(): \n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 1.3034, train acc 0.537, test acc 0.668, time 143.0 sec\n",
      "epoch 2, loss 0.8761, train acc 0.696, test acc 0.705, time 140.5 sec\n",
      "epoch 3, loss 0.7987, train acc 0.718, test acc 0.731, time 130.8 sec\n",
      "epoch 4, loss 0.7421, train acc 0.746, test acc 0.746, time 124.5 sec\n",
      "epoch 5, loss 0.6925, train acc 0.763, test acc 0.755, time 124.7 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "\n",
    "lr, num_epochs = 0.002, 5\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结NiN\n",
    "\n",
    "- 使用由卷积层和1×1卷积层构成的NiN块来构建深层网络。（1x1卷积层的作用类似于线性变换）\n",
    "- NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个acc好像不太行的样子..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
